## Кратко о сервисах

### Apache Airflow

**Apache Airflow** — открытое программное обеспечение для создания, выполнения, мониторинга и оркестровки потоков операций по обработке данных.
_Источник: [https://ru.wikipedia.org/wiki/Apache_Airflow](https://ru.wikipedia.org/wiki/Apache_Airflow)_

* Класс системы: ETL
* Документация: [https://airflow.apache.org/docs/](https://airflow.apache.org/docs/)
* Best practices: [https://airflow.apache.org/docs/apache-airflow/stable/best-practices.html](https://airflow.apache.org/docs/apache-airflow/stable/best-practices.html)

### Apache Superset

**Apache Superset** — открытое программное обеспечение для исследования и визуализации данных, ориентированное на большие данные.
_Источник: [https://ru.wikipedia.org/wiki/Apache_Superset](https://ru.wikipedia.org/wiki/Apache_Superset)_

* Класс системы: BI
* Документация: [https://superset.apache.org/docs/intro/](https://superset.apache.org/docs/intro/)

## Кратко о терминах

### ETL

**ETL** (от англ. Extract, Transform, Load — дословно «извлечение, преобразование, загрузка») — один из основных процессов в управлении хранилищами данных, ETL – общий термин для всех процессов миграции данных из одного источника в другой (другие связанные с этим термины – экспорт, импорт, конвертация данных, парсинг файлов, web-scrapping и пр.), который включает в себя:

Извлечение данных из внешних источников;
* их трансформация и очистка, чтобы они соответствовали потребностям бизнес-модели;
* и загрузка их в хранилище данных.

С точки зрения процесса ETL, архитектуру хранилища данных можно представить в виде трёх компонентов:

* источник данных: содержит структурированные данные в виде таблиц, совокупности таблиц или просто файла (данные в котором разделены символами-разделителями);
* промежуточная область: содержит вспомогательные таблицы, создаваемые временно и исключительно для организации процесса выгрузки.
* получатель данных: хранилище данных или база данных, в которую должны быть помещены извлечённые данные.

Перемещение данных от источника к получателю называют потоком данных. Требования к организации потока данных описываются аналитиком. ETL следует рассматривать не только как процесс переноса данных из одного приложения в другое, но и как инструмент подготовки данных к анализу.

Источник: [https://ru.wikipedia.org/wiki/ETL](https://ru.wikipedia.org/wiki/ETL)

### BI

**Business intelligence (BI)** — обозначение компьютерных методов и инструментов для организаций, обеспечивающих перевод транзакционной деловой информации в человекочитаемую форму, а также средства для массовой работы с такой обработанной информацией.

Цель BI — интерпретировать большое количество данных, заостряя внимание лишь на ключевых факторах эффективности, моделируя исход различных вариантов действий, отслеживая результаты принятия решений.

BI поддерживает множество бизнес-решений — от операционных до стратегических. Основные операционные решения включают в себя позиционирование продукта или цен. Стратегические бизнес-решения включают в себя приоритеты, цели и направления в самом широком смысле. BI наиболее эффективен, когда он объединяет данные, полученные из рынка, на котором работает компания (внешние данные), с данными из источников внутри компании, таких как финансовые и производственные (внутренние данные). В сочетании внешние и внутренние данные дают более полную картину бизнеса, или те самые «структурированные данные» (англ. intelligence) — аналитику, которую нельзя получить только от одного из этих источников.

Источник: [https://ru.wikipedia.org/wiki/Business_Intelligence](https://ru.wikipedia.org/wiki/Business_Intelligence)

## Управление сервисами

Для управления сервисами используется `docker` и `docker compose`.

Для удобства, сервисы разделены по директориям, поэтому их можно использовать отдельно.
Обратите внимание, что сервис `dataset_postgres` является вспомогательным и имеет доступ в обе сети -
`airflow_default` и `superset_default` для того, чтобы Airflow мог записать данные в него, а
Superset - прочитать и вывести. Есть много других путей для обмена данными, но этот я посчитал наиболее удобным и привычным
для примера.

### Первый запуск

```bash
# Убедитесь, что вы находитесь в директории superset_airflow_postgres
 
make init
make up
# Для первого запуска подождите значительное время,
# может потребоваться 5-10 минут.
#
# Ориентироваться можно по командам:
# make ps
# make logs
# чтобы понимать, что superset_init закончил работу.
#
# Если этого не произошло, то сделайте make destroy
# и заново make init и make up.
```

### Поднять и остановить сервисы

```bash
# Поднять все сервисы
make up

# Запустить все сервисы
make start

# Остановить все сервисы
make stop

# Удалить все сервисы
make down

# Перезапустить все сервисы
make restart
```

### Уничтожить все

```bash
# Если необходимо уничтожить все данные и настройки вплоть до .env
make destroy
```

## Доступы

### Superset

* [http://localhost:8088](http://localhost:8088) 
* Логин: admin
* Пароль: admin

### Airflow

* [http://localhost:8080](http://localhost:8080) 
* Логин: admin
* Пароль: admin

### Adminer (Postgres)

* [http://localhost:8044](http://localhost:8044) 
* Логин: admin
* Пароль: admin

## Прочее

Пример XML файла был взят отсюда: [https://www.w3schools.com/xml/cd_catalog.xml](https://www.w3schools.com/xml/cd_catalog.xml)
